{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## PART1 : SELF MADE NEURAL NETWORK"
      ],
      "metadata": {
        "id": "yuekVSKgL0Fk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f6MIHZI3RxM",
        "outputId": "0e305b6a-6b45-42a2-c76b-07a7eb337079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-29 18:06:14--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  51.0MB/s    in 3.5s    \n",
            "\n",
            "2023-04-29 18:06:18 (46.5 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "with tarfile.open('cifar-10-python.tar.gz', 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall('cifar-data')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "\n",
        "def correlate2DValid(A,B):\n",
        "  b1,b2=B.shape\n",
        "  a1,a2=A.shape\n",
        "  out=np.zeros((a1-b1+1,a2-b2+1))\n",
        "  for i in range (a1-b1+1):\n",
        "    for j in range (a2-b2+1):\n",
        "      out[i][j]=np.sum(A[i:i+b1,j:j+b2]*B)\n",
        "  return out\n",
        "# print correlate2D\n",
        "\n",
        "# def convFull(A,B):\n",
        "#   B = np.rot90(np.rot90(B))\n",
        "#   (a1,a2)=A.shape\n",
        "#   (b1,b2)=B.shape\n",
        "\n",
        "scalefactor=0.66\n",
        "\n",
        "def convolve2DFull(A, B):\n",
        "    B = np.rot90(np.rot90(B))\n",
        "    (b1,b2)=B.shape\n",
        "    padded_A = np.pad(A, [(b1-1, b1-1), (b2-1, b2-1)], mode='constant')\n",
        "    output_shape = (A.shape[0] + b1 - 1, A.shape[1] + b2 - 1)\n",
        "    output = np.zeros(output_shape)\n",
        "\n",
        "    for i in range(output_shape[0]):\n",
        "        for j in range(output_shape[1]):\n",
        "            output[i][j] = np.sum(padded_A[i:i+b1, j:j+b2] * B)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "MKAZds1s2AWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "dict1=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_1\")\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "dict1=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_1\")\n",
        "dict2=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_2\")\n",
        "dict3=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_3\")\n",
        "dict4=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_4\")\n",
        "dict5=unpickle(\"cifar-data/cifar-10-batches-py/data_batch_5\")\n",
        "\n",
        "\n",
        "\n",
        "xtrain= np.vstack((dict1[b\"data\"],dict2[b\"data\"],dict3[b\"data\"],dict4[b\"data\"],dict5[b\"data\"]))\n",
        "\n",
        "\n",
        "ytrain = np.hstack((np.array(dict1[b\"labels\"]),np.array(dict2[b\"labels\"]),np.array(dict3[b\"labels\"]),np.array(dict4[b\"labels\"]),np.array(dict5[b\"labels\"])))\n",
        "\n",
        "\n",
        "\n",
        "# Reshape xtrain to have one row per training example\n",
        "num_examples = xtrain.shape[0]\n",
        "print(xtrain.shape)\n",
        "\n",
        "xtrain = xtrain.reshape(num_examples, 3,32,32)\n",
        "\n",
        "print(xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "# Normalize the input features\n",
        "print(ytrain[0:50])\n",
        "# Convert pixel values to floats in the range 0-\n",
        "# xtrain=xtrain.astype('float32')\n",
        "xtrain = xtrain.astype('float32') / 255.0\n",
        "\n",
        "# # Compute mean and standard deviation of pixel values in training set\n",
        "# mean = np.mean(xtrain, axis=(0,1,2))\n",
        "# std = np.std(xtrain, axis=(0,1,2))\n",
        "\n",
        "# # Subtract mean from each image and divide by standard deviation\n",
        "# xtrain = (xtrain - mean) / std"
      ],
      "metadata": {
        "id": "S478jtrb6pT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79b4bd5-100a-4f71-90d4-e1b8f4db9c07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3072)\n",
            "(50000, 3, 32, 32)\n",
            "(50000,)\n",
            "[6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0 0 9 1 3 4 0 3\n",
            " 7 3 3 5 2 2 7 1 1 1 2 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def he_init(shape):\n",
        "    # shape is a tuple containing (n_in, n_out)\n",
        "    n_in, n_out = shape\n",
        "    # He initialization\n",
        "    stddev = np.sqrt(2/n_in)\n",
        "    W = np.random.normal(loc=0.0, scale=stddev, size=shape)\n",
        "    return W"
      ],
      "metadata": {
        "id": "UNNCRILIVbn8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "      # X and Y will be 1d nparrays\n",
        "        self.X = None\n",
        "        self.Y = None\n",
        "    \n",
        "    # def forwardProp(self, input):\n",
        "    #     raise NotImplementedError(\"This function has not been implemented here\")\n",
        "\n",
        "    # def backProp(self, output_error, learning_rate):\n",
        "    #     raise NotImplementedError(\"This function has not been implemented here\")"
      ],
      "metadata": {
        "id": "7gCH1XH7AT8j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "input = np.array([[1,2,2],[2,3,3]])\n",
        "B = np.array([1,2,0])\n",
        "C = np.array([1,0,1])\n",
        "D = np.array([[1,2,0],[1,0,1]])\n",
        "print(np.dot(input,input.T))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37jlgxpMVuFp",
        "outputId": "a3b101bc-1de8-4235-a072-e0465a8d09b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 9 14]\n",
            " [14 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatter(Layer):\n",
        "  #change is not needed here\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        # there must not be a change along the batch length dimension which is the zeroth dimension. \n",
        "        assert(input_shape[0] == output_shape[0])\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forwardProp(self, input):\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backProp(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self.input_shape)"
      ],
      "metadata": {
        "id": "wtBsmb903NnW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D(Layer):\n",
        "  # Assuming a square pool, can be changed easily.\n",
        "    def __init__(self,input_shape,pool_height,pool_width,pool_stride):\n",
        "      self.pool_height=pool_height\n",
        "      self.pool_width=pool_width\n",
        "      self.pool_stride=pool_stride\n",
        "      self.input_shape = input_shape\n",
        "      self.batch_size = input_shape[0]\n",
        "      self.input_depth=input_shape[1]\n",
        "      self.output_shape = (self.input_shape[0],self.input_shape[1],(int)(1+((self.input_shape[2] - pool_height)//pool_stride)),(int)(1+((self.input_shape[3] - pool_width) // pool_stride)))\n",
        "      \n",
        "\n",
        "    def forwardProp(self,input):\n",
        "\n",
        "      self.X=input\n",
        "\n",
        "      #this assumes that Layer has been initalised before forwardProp has been called. \n",
        "      assert(input.shape == self.input_shape)\n",
        "\n",
        "      # print(\"outshape is ,\",self.output_shape)\n",
        "      self.Y=np.zeros(self.output_shape)\n",
        "      for n in range (self.batch_size):\n",
        "        for d in range(self.input_depth):\n",
        "            for hi in range(self.output_shape[2]):\n",
        "                for wi in range(self.output_shape[3]):\n",
        "                    # print(\"pool height and width are : \", n,d,hi,wi,self.pool_height,self.pool_width)\n",
        "                    self.Y[n,d, hi, wi] = np.max(input[n,d, hi * self.pool_stride : hi * self.pool_stride + self.pool_height, wi * self.pool_stride : wi * self.pool_stride + self.pool_width]) \n",
        "      # print(self.Y.shape)\n",
        "      return self.Y\n",
        "\n",
        "    def backProp(self,output_error,learning_rate):\n",
        "          assert(output_error.shape==self.output_shape)\n",
        "          del_EX = np.zeros(self.input_shape)\n",
        "          \n",
        "          for n in range (self.batch_size):\n",
        "           for d in range(self.input_depth):\n",
        "                for i in range(self.output_shape[2]):\n",
        "                    for j in range(self.output_shape[3]):\n",
        "                        # get the index in the region i,j where the value is the maximum\n",
        "                        i_max, j_max = np.where(np.max(self.X[n,d, i * self.pool_stride : i * self.pool_stride + self.pool_height, j * self.pool_stride : j * self.pool_stride + self.pool_width]) == self.X[n,d, i * self.pool_stride : i * self.pool_stride + self.pool_height, j * self.pool_stride : j * self.pool_stride + self.pool_width])\n",
        "                        if(len(i_max)==0 or len(j_max)==0):\n",
        "                            print(self.X)\n",
        "                        i_max, j_max = i_max[0], j_max[0]\n",
        "                        # only the position of the maximum element in the region i,j gets the incoming gradient, the other gradients are zero\n",
        "                        del_EX[n,d, i * self.pool_stride : i * self.pool_stride + self.pool_height, j * self.pool_stride : j * self.pool_stride + self.pool_width][i_max, j_max] = output_error[n,d, i, j]\n",
        "          #  print(del_EX.shape)\n",
        "          assert(del_EX.shape==self.X.shape)\n",
        "          return del_EX\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lHFH4C0yeFkP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D(Layer):\n",
        "    def __init__(self, input_shape, kernel_side_length, num_kernels):\n",
        "      #kernel is always a sqaure , hence kernel_size tells the side length of the kernel.\n",
        "      # we start by unpacking the depth, height , width contained in the input\n",
        "        batch_size, input_depth, input_height, input_width = input_shape\n",
        "        # output depth will be num_kernels, each kernel gives one new output layer. \n",
        "        self.num_kernels = num_kernels\n",
        "        self.input_shape = input_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.input_depth = input_depth\n",
        "        self.output_shape = (self.batch_size,num_kernels, input_height - kernel_side_length + 1, input_width - kernel_side_length + 1)\n",
        "      # the first dimension tells number of kernels, the second dimension tells depth of kernel, this will be equal to input depth, the next two tell its side lengths\n",
        "        self.kernels_shape = (num_kernels, input_depth, kernel_side_length, kernel_side_length)\n",
        "        # stddev = np.sqrt(2.0/self.input_depth)\n",
        "        stddev=0.01\n",
        "        self.kernels = np.random.normal(loc=0.0, scale=stddev, size=self.kernels_shape)\n",
        "        self.Bias = np.random.normal(loc=0.0, scale=stddev, size=(num_kernels, input_height - kernel_side_length + 1, input_width - kernel_side_length + 1))\n",
        "        # self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        # self.Bias = np.random.randn(num_kernels, input_height - kernel_side_length + 1, input_width - kernel_side_length + 1)\n",
        "    def forwardProp(self,input):\n",
        "\n",
        "      #this assumes that Layer has been initalised before forwardProp has been called. \n",
        "      assert(input.shape == self.input_shape)\n",
        "\n",
        "      self.X=input\n",
        "      # self.Y = self.Bias\n",
        "\n",
        "      bias_4d = self.Bias[np.newaxis, :, :, :]\n",
        "\n",
        "# Repeat along the new axis\n",
        "      self.Y = np.repeat(bias_4d, self.batch_size, axis=0)\n",
        "\n",
        "      for k in range(self.batch_size):\n",
        "        for i in range(self.num_kernels):\n",
        "          for j in range(self.input_depth):\n",
        "            # self.Y[:, i] += correlate2DValid(self.X[:, j], self.kernels[i][j])\n",
        "\n",
        "            # need to add a an extra 0 dimension to this operation. \n",
        "            self.Y[k][i]+=correlate2DValid(self.X[k][j],self.kernels[i][j])\n",
        "      assert(self.Y.shape==self.output_shape)\n",
        "      return self.Y\n",
        "\n",
        "    def backProp(self,output_error,learning_rate):\n",
        "     # output_error will be a 1-d nparray  \n",
        "     #diversion\n",
        "     assert(output_error.shape==self.output_shape)\n",
        "     del_EK = np.zeros(self.kernels_shape)\n",
        "     del_EX = np.zeros(self.input_shape)\n",
        "\n",
        "     for k in range(self.batch_size):\n",
        "       for i in range(self.num_kernels):\n",
        "        for j in range(self.input_depth):\n",
        "        \n",
        "          #need to add extra dimension here as well, moreover for kernel you have to take mean over last dimension as well. \n",
        "            del_EK[i][j]+=correlate2DValid(self.X[k][j],output_error[k][i])\n",
        "            del_EX[k][j]+=convolve2DFull(output_error[k][i],self.kernels[i,j])\n",
        "    #  del_EK/=self.batch_size \n",
        "    # not dividing by mean intentionally. \n",
        "    #  del_EX/=self.batch_size\n",
        "\n",
        "          \n",
        "    #  for i in range(self.input_depth):\n",
        "    #    for j in range(self.num_kernels):\n",
        "    #      del_EK[j][i]=correlate2DValid(self.X[i],output_error[j])\n",
        "    #      del_EX[i] += convolve2DFull(output_error[j],self.kernels[j][i])\n",
        "\n",
        "     self.kernels-=learning_rate*del_EK\n",
        "     self.Bias-=learning_rate*np.sum(output_error,axis=0)\n",
        "     return del_EX\n"
      ],
      "metadata": {
        "id": "4gF40Lt7PhZb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X1 = np.array([2,3])\n",
        "X2 = np.array([3,4])\n",
        "Y1 = np.array([1,0,1]) \n",
        "Y2 = np.array ([5,10,5])\n",
        "print(np.outer(X1,Y1))\n",
        "print(np.outer(X2,Y2))\n",
        "X = np.array([[2,3],[3,4]])\n",
        "Y = np.array([[1,0,1],[5,10,5]])\n",
        "print(np.outer(X,Y))\n",
        "outer_products = X[:,:,np.newaxis] * Y[:,np.newaxis,:]\n",
        "\n",
        "# Take the mean along the first axis (rows of A)\n",
        "mean_outer_product = np.mean(outer_products, axis=0)\n",
        "print(mean_outer_product)\n",
        "# Flatten the result into a 1D array\n",
        "# result = mean_outer_product.flatten()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY7hBpNxrKGh",
        "outputId": "3a1f0b2a-9ef2-4e5f-bed5-298c0190fce1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0 2]\n",
            " [3 0 3]]\n",
            "[[15 30 15]\n",
            " [20 40 20]]\n",
            "[[ 2  0  2 10 20 10]\n",
            " [ 3  0  3 15 30 15]\n",
            " [ 3  0  3 15 30 15]\n",
            " [ 4  0  4 20 40 20]]\n",
            "[[ 8.5 15.   8.5]\n",
            " [11.5 20.  11.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu(Layer):\n",
        "  def forwardProp(self,input):\n",
        "    self.X=input\n",
        "    self.Y = np.maximum(input,0)\n",
        "    return self.Y\n",
        "  \n",
        "  def backProp(self,output_error,learning_rate):\n",
        "    return np.where(self.X < 0, 0, 1)*output_error\n",
        "  "
      ],
      "metadata": {
        "id": "3C5T0VfSLyuH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a layer without activation, to implement activation, you add another layer.\n",
        "class Linear(Layer):\n",
        "  def __init__(self,batch_size,inDimension,outDimension):\n",
        "    # W will be 2d np array\n",
        "    # self.W =he_init((inDimension, outDimension))\n",
        "    mean = 0\n",
        "    stddev = 0.01\n",
        "# Generate array of standard normal values\n",
        "    self.W = np.random.normal(loc=mean, scale=stddev, size=(inDimension,outDimension))\n",
        "\n",
        "    # Bias will be 1-d nparray\n",
        "    # diverison\n",
        "    self.Bias = np.random.normal(loc=mean, scale=stddev, size=(outDimension))\n",
        "    # self.Bias = 0.01*np.ones(outDimension)\n",
        "  \n",
        "  def forwardProp(self,input):\n",
        "    self.X=input\n",
        "    self.Y = np.dot(self.X,self.W)+self.Bias\n",
        "    # taken into account batch_size\n",
        "    return self.Y\n",
        "\n",
        "  def backProp(self,output_error,learning_rate):\n",
        "    # output_error will be a 1-d nparray  \n",
        "    #diversion\n",
        "    # print(\"outshape is\",output_error.shape)\n",
        "    # print(\"inputshape is\",self.X.shape)\n",
        "    outer_products = self.X[:,:,np.newaxis] * output_error[:,np.newaxis,:]\n",
        "    del_EW = np.sum(outer_products, axis=0)\n",
        "    assert(del_EW.shape == self.W.shape)\n",
        "\n",
        "    \n",
        "    # del_EX= np.dot(self.W.T,output_error)\n",
        "    del_EX= (np.dot(output_error,self.W.T))\n",
        "    assert(self.X.shape==del_EX.shape)\n",
        "    # update parameters\n",
        "    self.W -= learning_rate * del_EW\n",
        "    self.Bias -= learning_rate * np.sum(output_error,axis=0)\n",
        "    return del_EX"
      ],
      "metadata": {
        "id": "-B0yPWaBCPR4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def cross_entropy(y_true,y_pred):\n",
        "  # print(\"Y_pred obtained is \",y_pred)\n",
        "  max_per_row = np.max(y_pred, axis=1)\n",
        "\n",
        "# subtract maximum element from each element in the corresponding row\n",
        "  y_pred = y_pred - max_per_row[:, np.newaxis]\n",
        "\n",
        "  # print(\"y_pred is \",y_wepred)\n",
        "  e_sum = np.sum(np.exp(y_pred),axis=1)\n",
        "  # print(\"e_sum is \",e_sum)\n",
        "  b = np.exp(y_pred[np.arange(len(y_true)), y_true])\n",
        "  # print(b.shape)\n",
        "  losses= -np.log2(((b)/(e_sum))+1e-100)\n",
        "  # print(losses.shape)\n",
        "  # print(losses)\n",
        "  return np.mean(losses)\n",
        "  # Convert the true class labels to a 2D array of shape (num_samples, num_classes) using one-hot encoding\n",
        "  \n",
        "\n",
        "def cross_entropy_derivative(y_true,y_pred):\n",
        "\n",
        "  # print(\"y_pred obtained at Cross_derivative : \",y_pred)\n",
        "  max_per_row = np.max(y_pred, axis=1)\n",
        "\n",
        "# subtract maximum element from each element in the corresponding row\n",
        "  y_pred = y_pred - max_per_row[:, np.newaxis]\n",
        "  e_sum = np.sum(np.exp(y_pred),axis=1)\n",
        "  d_y = np.exp(y_pred) / e_sum[:, np.newaxis]\n",
        "  for i, j in enumerate(y_true):\n",
        "    d_y[i, j] -= 1\n",
        "  # print(\"y_true is ,\",y_true,\"and error that I am back propogating is \",d_y)\n",
        "  return d_y\n"
      ],
      "metadata": {
        "id": "xknqdBVVhC9l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(cross_entropy(np.array([2,1,4,2])),np.array([[0,0,100,0,0],[100,2,1,1,1],[0.1,0.2,0.3,0.1,0.3],[0.4,0.4,0.1,0.05,0.05]]))"
      ],
      "metadata": {
        "id": "j7DcJI_X7sjn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=32"
      ],
      "metadata": {
        "id": "46qTIy0MnSqB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forwardProp(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
        "    for e in range(epochs):\n",
        "        error = 0\n",
        "        iter=0\n",
        "        for i in tqdm(range(0,(50000//batch)*batch,batch)):\n",
        "            x=x_train[i:i+batch]\n",
        "            y=y_train[i:i+batch]\n",
        "            # forward\n",
        "            output = predict(network, x)\n",
        "            iter+=1\n",
        "            # error\n",
        "            l = loss(y, output)\n",
        "            error += l\n",
        "            print(\"loss in iter, \",iter , \" is \",l*scalefactor)\n",
        "            # backward\n",
        "            grad = loss_prime(y, output)\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backProp(grad, learning_rate)\n",
        "\n",
        "        error /= len(x_train)\n",
        "        if verbose:\n",
        "            print(f\"{e + 1}/{epochs}, error={error}\")\n"
      ],
      "metadata": {
        "id": "7FgINCKlmT2e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_loss_derivative(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "SwrvkMQfPMjp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# neural network\n",
        "network = [\n",
        "    Conv2D((batch,3, 32, 32),3,32),\n",
        "    Relu(),\n",
        "    MaxPool2D((batch,32,30,30),2,2,2),\n",
        "    Conv2D((batch,32, 15, 15), 5,64),\n",
        "    Relu(),\n",
        "    MaxPool2D((batch,64,11,11),2,2,2),\n",
        "    Conv2D((batch,64, 5, 5), 3 ,64),\n",
        "    Relu(),\n",
        "    Flatter((batch,64, 3, 3), (batch,576)),\n",
        "    Linear(batch,576, 64),\n",
        "    Relu(),\n",
        "    Linear(batch,64, 10)\n",
        "    # Softmax()\n",
        "]\n",
        "\n",
        "# network = [\n",
        "#     # MaxPool2D((batch,3,32,32),2,2,2),\n",
        "#     Flatter((batch,3, 32, 32), (batch,3072)),\n",
        "#     Linear(batch,3072, 256),\n",
        "#     # Relu(),\n",
        "#     Linear(batch,256, 64),\n",
        "#     # Relu(),\n",
        "#     Linear(batch,64, 10)\n",
        "#     # Softmax()\n",
        "# ]\n",
        "\n",
        "# network = [\n",
        "#     MaxPool2D((batch,3,32,32),2,2,2),\n",
        "#     Flatter((batch,3,16,16),(batch,768)),\n",
        "#     Linear(batch,768,10),\n",
        "#     Relu()\n",
        "# ] \n",
        "\n",
        "# train\n",
        "train(\n",
        "    network,\n",
        "    cross_entropy,\n",
        "    cross_entropy_derivative,\n",
        "    xtrain,\n",
        "    ytrain,\n",
        "    epochs=20,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "\n",
        "# test\n",
        "for x, y in zip(x_test, y_test):\n",
        "    output = predict(network, x)\n",
        "    print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")"
      ],
      "metadata": {
        "id": "GT_JON283o88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}